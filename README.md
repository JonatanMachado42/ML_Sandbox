# Plano de IA

Esse √© o plano de estudos completo para minhas f√©rias de ver√£o de 2026, meu foco √© restruturar completamente os meus estudos de IA que ficaram desorganizados pelos ultimos meses.

Objetivo: Construir intui√ß√£o matem√°tica formal implementando algoritmos do zero (NumPy) e ganhar profici√™ncia em ferramentas de mercado (PyTorch/Scikit-Learn).

---

## üìÇ Estrutura do Reposit√≥rio

*OBS: Organize seu c√≥digo assim desde o Dia 1. N√£o use notebooks para c√≥digo final.*


```
ai-studies/
‚îú‚îÄ‚îÄ README.md               # Seu di√°rio de bordo.
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias (numpy, pandas, torch, black, pytest).
‚îú‚îÄ‚îÄ data/                   # (.gitignore) Dados brutos e processados.
‚îú‚îÄ‚îÄ notebooks/              # Apenas para explora√ß√£o e gr√°ficos (Rascunho).
‚îú‚îÄ‚îÄ src/                    # C√≥digo de Produ√ß√£o (Classes, Fun√ß√µes).
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Suas implementa√ß√µes (knn.py, linear_reg.py, mlp.py).
‚îÇ   ‚îú‚îÄ‚îÄ metrics/            # Suas m√©tricas (accuracy.py, f1.py).
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/      # Normalizadores e DataLoaders.
‚îÇ   ‚îî‚îÄ‚îÄ visualization/      # Plots de curvas de aprendizado.
‚îî‚îÄ‚îÄ tests/                  # Testes unit√°rios (test_linear.py, test_mlp.py).
```

---

##  M√≥dulo 1: Matem√°tica & Algoritmos Cl√°ssicos (NumPy Puro)

**Restri√ß√µes:** Proibido usar Scikit-Learn para modelos. Apenas NumPy, Pandas e Matplotlib.

### Subm√≥dulo 1.1: O In√≠cio (KNN & M√©tricas)

*Foco: Entender dist√¢ncias vetoriais e como medir sucesso.*

- [ ] **Engenharia de Software**
  
  - [ ] Configurar ambiente (venv), Git e instalar `black` (linter) e `pytest`.
    
  - [ ] Criar estrutura de pastas.
    
- [ ] **Modelo: K-Nearest Neighbors (KNN)**
  
  - [ ] Criar classe `KNNClassifier` em `src/models/knn.py`.
    
  - [ ] Implementar `.fit(X, y)` (Apenas armazena dados).
    
  - [ ] Implementar `.predict(X)` usando **Broadcasting do NumPy** para calcular Dist√¢ncia Euclidiana ($\sqrt{\sum(x-y)^2}$) sem loops `for`.
    
- [ ] **M√©tricas (Do Zero)**
  
  - [ ] Implementar `accuracy_score`.
    
  - [ ] Implementar `confusion_matrix` (Entender TP, TN, FP, FN).
    

### Subm√≥dulo 1.2: O Motor da Otimiza√ß√£o (Regress√£o Linear)

*Foco: O algoritmo mais importante de todos (Gradiente Descendente).*

- [ ] **Pr√©-processamento**
  
  - [ ] Implementar **Normaliza√ß√£o (MinMax)** e **Padroniza√ß√£o (Z-Score)** em `src/preprocessing`.
    
  - [ ] *Conceito:* Testar treinar sem normalizar e ver o modelo falhar.
    
- [ ] **Modelo: Regress√£o Linear**
  
  - [ ] Criar classe `LinearRegression`.
    
  - [ ] Implementar o loop de **Gradiente Descendente** ($\theta = \theta - \alpha \cdot \nabla J$), usando Batches.
    
  - [ ] Criar teste unit√°rio comparando seus pesos finais com `sklearn.linear_model.LinearRegression`.
    
- [ ] **Avalia√ß√£o**
  
  - [ ] Implementar **MSE** (Mean Squared Error) e **R¬≤**.
    
  - [ ] Plotar a curva de Loss x √âpocas (para ver o aprendizado acontecer).
    

### Subm√≥dulo 1.3: Classifica√ß√£o Probabil√≠stica (Regress√£o Log√≠stica)

*Foco: Probabilidade e Fronteiras de Decis√£o.*

- [ ] **Modelo: Regress√£o Log√≠stica**
  
  - [ ] Implementar fun√ß√£o **Sigmoid** ($1 / (1 + e^{-z})$).
    
  - [ ] Implementar **Log-Loss / Binary Cross-Entropy** (A fun√ß√£o de custo para Sim/N√£o).
    
- [ ] **M√©tricas Avan√ßadas**
  
  - [ ] Implementar **Precision**, **Recall** e **F1-Score** (Crucial para dados desbalanceados).
    
  - [ ] *Desafio:* Entender o Trade-off: aumentar o threshold aumenta Precision mas diminui Recall.
    

### Subm√≥dulo 1.4: N√£o Supervisionado (Clustering)

*Foco: Trabalhar sem labels.*

- [ ] **Modelo: K-Means**
  
  - [ ] Implementar inicializa√ß√£o de centr√≥ides.
    
  - [ ] Implementar loop: 1. Atribuir pontos ao centr√≥ide mais pr√≥ximo; 2. Mover centr√≥ide para a m√©dia dos pontos.
    
  - [ ] Gerar um GIF ou plot mostrando os centr√≥ides "andando" at√© o centro dos clusters.
    

---

##  M√≥dulo 2: Deep Learning & A Transi√ß√£o (NumPy -> PyTorch)

**Foco:** Sair da estat√≠stica linear e entrar na n√£o-linearidade (Redes Neurais).

### Subm√≥dulo 2.1: O Chef√£o Final do NumPy (MLP)

*O desafio mais dif√≠cil do curso.*

- [ ] **Matem√°tica "Hardcore"**
  
  - [ ] Entender a **Regra da Cadeia** (Chain Rule) para derivadas compostas.
- [ ] **Modelo: Multi-Layer Perceptron (Do Zero)**
  
  - [ ] Criar classe `NeuralNetwork`.
    
  - [ ] Implementar **Forward Pass** (Multiplica√ß√£o de Matrizes + Ativa√ß√£o ReLU).
    
  - [ ] Implementar **Backpropagation** (Calcular gradientes camada por camada voltando do fim para o come√ßo).
    
  - [ ] Resolver o problema do **XOR** (Dataset n√£o linear).
    
- [ ] **Conceitos de Treino**
  
  - [ ] Implementar suporte a **Mini-Batches** (N√£o passar o dataset inteiro de uma vez).

### Subm√≥dulo 2.2: Bem-vindo ao PyTorch

*Foco: Traduzir o que voc√™ fez acima para a ferramenta profissional.*

- [ ] **Ferramental**
  
  - [ ] Entender **Tensores** (GPU) e **Autograd** (Derivada Autom√°tica - adeus regra da cadeia manual).
- [ ] **Refatora√ß√£o**
  
  - [ ] Recriar a MLP acima usando `torch.nn.Linear` e `torch.optim.Adam`.
    
  - [ ] Comparar a velocidade de converg√™ncia do seu otimizador manual vs Adam.
    

### Subm√≥dulo 2.3: Vis√£o Computacional (CNNs)

*Foco: Processamento de Imagens e Dados Espaciais.*

- [ ] **Teoria & Pr√°tica**
  
  - [ ] Entender **Convolu√ß√£o** (Filtros) e **Pooling**.
    
  - [ ] Treinar uma CNN simples no dataset MNIST (D√≠gitos).
    
- [ ] **Engenharia de Dados (Cr√≠tico)**
  
  - [ ] Criar classes `Dataset` e `DataLoader` customizadas no PyTorch.
    
  - [ ] Implementar **Data Augmentation** (Rotacionar/Inverter imagens para evitar overfitting).
    
  - [ ] Salvar o melhor modelo durante o treino (`Model Checkpointing`).
    

---

##  M√≥dulo 3: O Mundo Real (Finan√ßas, NLP & SOTA)

**Foco:** Ferramentas de produ√ß√£o e arquiteturas modernas.

### Subm√≥dulo 3.1: Dados Tabulares & Finan√ßas (Ensembles)

*Foco: O que ganha competi√ß√µes no Kaggle e domina o mercado financeiro.*

- [ ] **Teoria (Sem implementar do zero)**
  
  - [ ] Entender √Årvores de Decis√£o (Entropia).
    
  - [ ] Entender Bagging (Random Forest) vs Boosting (XGBoost/LightGBM).
    
- [ ] **Pr√°tica Financeira**
  
  - [ ] Baixar dados de a√ß√µes (Yahoo Finance).
    
  - [ ] Treinar um **XGBoost** para prever dire√ß√£o do mercado.
    
  - [ ] **Preven√ß√£o de Erros:** Garantir valida√ß√£o temporal (Time Series Split) e n√£o aleat√≥ria.
    
  - [ ] **Explicabilidade:** Usar **SHAP Values** para entender quais features o modelo usou.
    

### Subm√≥dulo 3.2: S√©ries Temporais (RNNs/LSTMs)

*Foco: Mem√≥ria e Sequ√™ncias.*

- [ ] **Engenharia de Features**
  
  - [ ] Implementar **Janelamento (Sliding Window)** para transformar s√©rie temporal em aprendizado supervisionado.

- [ ] **Modelo: LSTM/GRU (PyTorch)**
  
  - [ ] Implementar uma rede recorrente para prever o pr√≥ximo valor da s√©rie.
    
  - [ ] Comparar performance: LSTM (Deep Learning) vs XGBoost (M√≥dulo 3.1).
    

### Subm√≥dulo 3.3: NLP Moderno & Transformers

*Foco: A arquitetura do ChatGPT.*

- [ ] **Matem√°tica da Aten√ß√£o**
  
  - [ ] Implementar a f√≥rmula de **Self-Attention** ($Attention(Q, K, V)$) com NumPy/PyTorch (apenas a fun√ß√£o, n√£o a rede toda).
- [ ] **Ecossistema Hugging Face**
  
  - [ ] Usar a biblioteca `transformers`.
    
  - [ ] Carregar um modelo pr√©-treinado (BERTimbau - BERT em PT).
    
  - [ ] Fazer **Fine-Tuning** para classifica√ß√£o de sentimentos em manchetes financeiras.
    

### Subm√≥dulo 3.4: Finaliza√ß√£o & Deploy B√°sico

- [ ] **Ciclo de Vida**
  
  - [ ] Criar script `inference.py` que carrega o modelo salvo (`.pt` ou `.pkl`) e recebe novos dados.

  - [ ] (Opcional) Containerizar com **Docker** simples.
    

---

###  Defini√ß√£o de Conclu√≠do (DoD)

Ao final de cada subm√≥dulo, voc√™ deve ter:

1. O arquivo `.py` do modelo na pasta `src/models`.  
2. Um script de teste em `tests/` que passa ("Green").
